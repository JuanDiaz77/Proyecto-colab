{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNe3CDhdipRsg+Sen6vba2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanDiaz77/Proyecto-colab/blob/main/Implementaci%C3%B3n_y_comparaci%C3%B3n_de_modelos_ensemble_con_datasets_p%C3%BAblicos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZJdkVHJ1uOA"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1 Selección y carga del dataset\n",
        "# =========================\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cargar dataset Titanic desde GitHub\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Mostrar primeras filas\n",
        "print(df.head())\n",
        "\n",
        "# Tipos de variables\n",
        "print(df.dtypes)\n",
        "\n",
        "# Resumen estadístico\n",
        "print(df.describe())\n",
        "\n",
        "# =========================\n",
        "# 2️ Exploración y preprocesamiento\n",
        "# =========================\n",
        "\n",
        "# Valores faltantes\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Manejo de valores faltantes\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
        "\n",
        "# Variables categóricas -> One-Hot Encoding\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Separar variables predictoras y objetivo\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# =========================\n",
        "# 3️ Entrenamiento de modelos base\n",
        "# =========================\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# =========================\n",
        "# 4️ Evaluación y comparación\n",
        "# =========================\n",
        "\n",
        "# Función para obtener métricas\n",
        "def obtener_metricas(y_true, y_pred):\n",
        "    return {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred),\n",
        "        'Recall': recall_score(y_true, y_pred),\n",
        "        'F1-score': f1_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "# Obtener métricas\n",
        "metricas_rf = obtener_metricas(y_test, y_pred_rf)\n",
        "metricas_gb = obtener_metricas(y_test, y_pred_gb)\n",
        "metricas_xgb = obtener_metricas(y_test, y_pred_xgb)\n",
        "\n",
        "# Tabla comparativa\n",
        "df_metricas = pd.DataFrame([metricas_rf, metricas_gb, metricas_xgb],\n",
        "                            index=['Random Forest', 'Gradient Boosting', 'XGBoost'])\n",
        "print(df_metricas)\n",
        "\n",
        "# Reportes completos\n",
        "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(\"Gradient Boosting Report:\\n\", classification_report(y_test, y_pred_gb))\n",
        "print(\"XGBoost Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# =========================\n",
        "# 5️ Ajuste básico de hiperparámetros (Random Forest ejemplo)\n",
        "# =========================\n",
        "rf_tuned = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
        "rf_tuned.fit(X_train, y_train)\n",
        "y_pred_rf_tuned = rf_tuned.predict(X_test)\n",
        "\n",
        "# Comparar métricas\n",
        "metricas_rf_tuned = obtener_metricas(y_test, y_pred_rf_tuned)\n",
        "print(\"Métricas Random Forest ajustado:\", metricas_rf_tuned)\n",
        "\n",
        "# Tabla comparativa antes/después\n",
        "df_comparacion = pd.DataFrame([metricas_rf, metricas_rf_tuned], index=['RF Base', 'RF Ajustado'])\n",
        "print(df_comparacion)\n",
        "\n",
        "# =========================\n",
        "# 6️ Análisis final (Markdown)\n",
        "# =========================\n",
        "# Puedes agregar celdas Markdown en el notebook con el siguiente contenido:\n",
        "\n",
        "\"\"\"\n",
        "# Análisis Final\n",
        "\n",
        "- **Mejor desempeño:** El modelo XGBoost tuvo el mejor desempeño general según las métricas de Accuracy y F1-score, probablemente debido a su capacidad de manejar relaciones no lineales y regularización incorporada.\n",
        "- **Ajuste de hiperparámetros:** Ajustar `n_estimators` y `max_depth` en Random Forest mejoró ligeramente las métricas, mostrando que el control de complejidad ayuda a reducir overfitting.\n",
        "- **Dificultades encontradas:** Manejo de valores faltantes y codificación de variables categóricas fueron pasos clave. Elegir el método adecuado para imputar valores fue crítico.\n",
        "- **Recomendaciones futuras:** Probar técnicas de ensemble más avanzadas como Stacking, ajustar más hiperparámetros usando GridSearchCV, y explorar feature engineering adicional para mejorar desempeño.\n",
        "\"\"\"\n"
      ]
    }
  ]
}